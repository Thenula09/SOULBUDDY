"""
SoulBuddy Chat AI Service - Groq-powered emotional chat buddy
"""
import os
import json
import base64
import shutil
import tempfile
from fastapi import FastAPI, HTTPException, Depends, Header, File, UploadFile
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from groq import Groq
from typing import Optional
from dotenv import load_dotenv
import cv2
import numpy as np
from PIL import Image
import shutil
import base64

# Try to import DeepFace for AI-powered emotion detection
DEEPFACE_AVAILABLE = False
try:
    from deepface import DeepFace
    print("‚úÖ DeepFace emotion detector initialized successfully")
    DEEPFACE_AVAILABLE = True
except Exception as e:
    print(f"‚ö†Ô∏è DeepFace not available: {e}")
    print("‚ÑπÔ∏è  Using fallback emotion detection")

load_dotenv()

def detect_emotion_with_deepface(image_path):
    """
    üé≠ AI-Powered Emotion Detection using DeepFace
    Returns: (emotion, confidence, all_emotions_dict)
    """
    if not DEEPFACE_AVAILABLE:
        print("‚ö†Ô∏è DeepFace not available, using fallback")
        return "Neutral", 0.4, {}
    
    try:
        # Analyze with DeepFace
        result = DeepFace.analyze(
            img_path=image_path,
            actions=['emotion'],
            enforce_detection=False,  # Don't fail if no face detected
            detector_backend='opencv',  # Use OpenCV for speed
            silent=True
        )
        
        if isinstance(result, list):
            result = result[0]
        
        # Get emotions
        emotions = result['emotion']
        dominant_emotion = result['dominant_emotion']
        
        # Map DeepFace emotions to our app emotions
        emotion_mapping = {
            'happy': 'Happy',
            'sad': 'Sad',
            'angry': 'Angry',
            'fear': 'Anxious',
            'surprise': 'Excited',
            'neutral': 'Neutral',
            'disgust': 'Stressed'
        }
        
        app_emotion = emotion_mapping.get(dominant_emotion, 'Neutral')
        confidence = emotions[dominant_emotion] / 100.0
        
        print(f"‚úÖ DeepFace: {app_emotion} ({confidence:.2f})")
        print(f"   All emotions: {emotions}")
        
        return app_emotion, confidence, emotions
        
    except Exception as e:
        print(f"‚ö†Ô∏è DeepFace error: {e}")
        return "Neutral", 0.4, {}

app = FastAPI(title="SoulBuddy Chat AI Service", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize Groq client
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
if not GROQ_API_KEY:
    print("‚ö†Ô∏è  GROQ_API_KEY not found in environment. Chat service will not work properly.")
    groq_client = None
else:
    groq_client = Groq(api_key=GROQ_API_KEY)
    print("‚úÖ Groq client initialized successfully")

# Initialize OpenCV Face Detector (for emotion analysis)
try:
    # Load Haar Cascade for face detection
    face_cascade_path = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
    face_cascade = cv2.CascadeClassifier(face_cascade_path)
    print("‚úÖ OpenCV face detector initialized successfully")
except Exception as e:
    print(f"‚ö†Ô∏è  Face detector initialization failed: {e}")
    face_cascade = None

class ChatRequest(BaseModel):
    message: str
    user_id: Optional[int] = None

class EmotionRequest(BaseModel):
    image: str  # Base64 encoded image

class ChatResponse(BaseModel):
    reply: str
    emotion: str
    confidence: Optional[float] = None

# System prompt for SoulBuddy personality
SYSTEM_PROMPT = """You are SoulBuddy, a deeply caring best friend from Sri Lanka who asks meaningful questions.

üéØ CRITICAL RULES:
1. ALWAYS respond in the EXACT SAME LANGUAGE the user uses
   - English input ‚Üí English response
   - Sinhala (‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω) input ‚Üí Sinhala response  
   - Singlish (mixed) input ‚Üí Singlish response
   
2. ASK DEEP FOLLOW-UP QUESTIONS like a real friend:
   - If they're SAD: Don't just say "sorry to hear that" - dig deeper!
     Ask WHY they're sad ("What happened, buddy?", "Did someone say something hurtful?", "Wanna talk about it?")
   - If they're HAPPY: Share their joy and ask what made them so happy
     ("That's awesome! What happened?", "Tell me more!", "What made your day?")
   - If they're STRESSED: Ask what's causing the stress
     ("What's stressing you out?", "Is it work/school?", "Anything I can help with?")
   - ALWAYS end with a caring question to keep conversation going
   
3. USE EMOJIS to express emotions naturally üòä
   - Add relevant emojis (üòä üéâ üí™ ‚ù§Ô∏è üò¢ üî• ‚ú® ü´Ç üí≠ etc.)
   - Use them like texting a close friend
   - 1-2 emojis per message is perfect
   
4. Detect emotions SILENTLY in the background
   - Analyze their emotional state from their message
   - Include emotion in JSON but don't mention it in your reply

üö® CRITICAL SAFETY GUARDRAILS (NON-NEGOTIABLE):
1. NEVER prescribe medicine or medical treatments
   - If asked about medicine/health: "I'm not a doctor, please consult a healthcare professional"
   - Sinhala: "‡∂∏‡∂∏ ‡∑Ä‡∑õ‡∂Ø‡∑ä‚Äç‡∂∫‡∑Ä‡∂ª‡∂∫‡∑ô‡∂ö‡∑ä ‡∂±‡∑ô‡∂∏‡∑ô‡∂∫‡∑í, ‡∂ö‡∂ª‡∑î‡∂´‡∑è‡∂ö‡∂ª ‡∑Ä‡∑õ‡∂Ø‡∑ä‚Äç‡∂∫‡∑Ä‡∂ª‡∂∫‡∑ô‡∂ö‡∑ä ‡∑Ñ‡∂∏‡∑î‡∑Ä‡∂±‡∑ä‡∂±"
   
2. SELF-HARM / SUICIDE mentions - IMMEDIATELY respond seriously:
   - Keywords: kill, suicide, die, ‡∂∏‡∑ê‡∂ª‡∑ô‡∂±‡∑ä‡∂±, ‡∂¢‡∑ì‡∑Ä‡∑í‡∂≠‡∑ö ‡∂â‡∑Ä‡∂ª
   - English: "Your life is valuable. Please call 1926 (Mental Health Helpline) or see a counselor. I'm here, but please get professional help. ü´Ç"
   - Sinhala: "‡∂î‡∂∫‡∑è‡∂ú‡∑ö ‡∂¢‡∑ì‡∑Ä‡∑í‡∂≠‡∂∫ ‡∂Ö‡∂ú‡∂±‡∑ö. ‡∂ö‡∂ª‡∑î‡∂´‡∑è‡∂ö‡∂ª 1926 ‡∂Ö‡∂∏‡∂≠‡∂±‡∑ä‡∂± ‡∑Ñ‡∑ù ‡∂ã‡∂¥‡∂Ø‡∑ö‡∑Å‡∂ö‡∂∫‡∑ô‡∂ö‡∑î ‡∑Ñ‡∂∏‡∑î‡∑Ä‡∂±‡∑ä‡∂±. ‡∂∏‡∂∏ ‡∂î‡∂∫‡∑è ‡∑É‡∂∏‡∂ü ‡∑É‡∑í‡∂ß‡∑í‡∂∏‡∑í, ‡∂±‡∂∏‡∑î‡∂≠‡∑ä ‡∑Ä‡∑ò‡∂≠‡∑ä‡∂≠‡∑ì‡∂∫ ‡∑É‡∑Ñ‡∑è‡∂∫ ‡∂Ω‡∂∂‡∑è ‡∂ú‡∂±‡∑ä‡∂±. ü´Ç"
   
3. For serious health/mental issues ‚Üí suggest professional help (doctor, therapist, 1926)

Your personality:
- Warm, caring, genuinely curious about their feelings
- Keep responses conversational (2-3 sentences) - like texting
- Be encouraging and supportive
- ALWAYS ask a relevant follow-up question
- BUT prioritize user safety above all else

Emotion Detection (silent - for JSON only):
Classify as: Happy, Sad, Angry, Stress, Neutral, Anxious, Excited

Response format (ALWAYS valid JSON):
{
  "reply": "your caring response with follow-up question and emojis",
  "emotion": "detected_emotion"
}

Examples:
User: "What's up macho" ‚Üí {"reply": "Hey! Just chilling here üòä What about you?", "emotion": "Neutral"}
User: "I got the job!" ‚Üí {"reply": "Yo that's amazing! Congrats buddy! üéâüî•", "emotion": "Happy"}
User: "‡∂ö‡∑ú‡∑Ñ‡∑ú‡∂∏‡∂Ø ‡∂î‡∂∫‡∑è" ‚Üí {"reply": "‡∂∏‡∂Ç ‡∑Ñ‡∑ú‡∂≥‡∑í‡∂±‡∑ä! ‡∂î‡∂∫‡∑è ‡∂ö‡∑ú‡∑Ñ‡∑ú‡∂∏‡∂Ø ‡∂∏‡∂†‡∂Ç? üòä", "emotion": "Neutral"}
User: "‡∂∏‡∂Ç ‡∂Ö‡∂Ø ‡∑Ñ‡∂ª‡∑í‡∂∏ ‡∑É‡∂≠‡∑î‡∂ß‡∑î‡∂∫‡∑í" ‚Üí {"reply": "‡∂Ö‡∂±‡∑ö ‡∑É‡∑î‡∂¥‡∑í‡∂ª‡∑í! ‡∂∏‡∑ú‡∂ö‡∂Ø ‡∑Ä‡∑î‡∂´‡∑ö ‡∂ö‡∑í‡∂∫‡∂±‡∑ä‡∂±? üéâ", "emotion": "Happy"}
User: "I failed my exam" ‚Üí {"reply": "Aw man, that sucks üòî But hey, one exam doesn't define you. You got this next time! üí™", "emotion": "Sad"}
User: "yako mata stress job eka gana" ‚Üí {"reply": "Oya relax wenna try karanna macho üòå Work stress normal ekak. Mokak hari issue ekak thiyanawada?", "emotion": "Stress"}
"""

async def verify_token(authorization: Optional[str] = Header(None)):
    """Simple token verification (optional for now)"""
    if not authorization:
        # For now, allow requests without token for testing
        return None
    # TODO: Add proper JWT verification if needed
    return authorization

@app.get("/")
async def root():
    return {
        "service": "SoulBuddy Chat AI Service", 
        "version": "1.0.0", 
        "status": "running",
        "groq_connected": groq_client is not None
    }

@app.get("/health")
async def health():
    return {
        "status": "healthy",
        "groq_available": groq_client is not None
    }

@app.post("/chat", response_model=ChatResponse)
async def chat(
    request: ChatRequest,
    authorization: Optional[str] = Depends(verify_token)
):
    """Main chat endpoint - processes user message and returns AI response with emotion"""
    
    if not groq_client:
        raise HTTPException(
            status_code=503,
            detail="Chat service not configured. Please set GROQ_API_KEY."
        )
    
    if not request.message or len(request.message.strip()) == 0:
        raise HTTPException(
            status_code=400,
            detail="Message cannot be empty"
        )
    
    try:
        # Call Groq API with updated model
        completion = groq_client.chat.completions.create(
            model="llama-3.1-8b-instant",  # Latest fast model
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": request.message}
            ],
            response_format={"type": "json_object"},
            temperature=0.7,  # Balanced creativity
            max_tokens=300,   # Increased for better JSON completion
        )
        
        # Parse AI response
        ai_response = completion.choices[0].message.content
        response_data = json.loads(ai_response)
        
        # Validate response structure
        if "reply" not in response_data or "emotion" not in response_data:
            raise ValueError("Invalid response format from AI")
        
        return ChatResponse(
            reply=response_data["reply"],
            emotion=response_data["emotion"],
            confidence=None  # Groq doesn't provide confidence scores
        )
        
    except json.JSONDecodeError as e:
        print(f"JSON decode error: {e}")
        print(f"Raw AI response: {ai_response}")
        # Fallback response
        return ChatResponse(
            reply="Hey, I'm here for you! Can you tell me more?",
            emotion="Neutral"
        )
    except Exception as e:
        print(f"Chat error: {e}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to process chat: {str(e)}"
        )

@app.get("/api/chat/test")
async def test_chat():
    """Test endpoint to verify chat service"""
    if not groq_client:
        return {
            "status": "error",
            "message": "Groq client not initialized. Check GROQ_API_KEY."
        }
    return {
        "status": "ok",
        "message": "Chat service is ready!",
        "model": "llama-3.1-8b-instant"
    }

@app.post("/analyze-emotion")
async def analyze_emotion(request: EmotionRequest):
    """
    üéØ Emotion Detection using Groq LLM + OpenCV Face Detection
    Accepts JSON with base64 encoded image
    Returns: detected emotion from facial image
    """
    temp_path = None
    
    try:
        # 1. Decode base64 image
        try:
            # Remove data URL prefix if present (data:image/jpeg;base64,...)
            base64_image = request.image
            if ',' in base64_image:
                base64_image = base64_image.split(',')[1]
            
            image_data = base64.b64decode(base64_image)
        except Exception as e:
            print(f"‚ùå Base64 decode error: {str(e)}")
            return {
                "emotion": "Neutral",
                "confidence": 0.3,
                "status": "fallback_decode_error",
                "error": "Invalid base64 image data"
            }
        
        # 2. Save to temp file
        temp_path = f"/tmp/emotion_analysis_{os.urandom(8).hex()}.jpg"
        
        with open(temp_path, "wb") as f:
            f.write(image_data)
        
        # 2. Detect face using OpenCV
        img = cv2.imread(temp_path)
        if img is None:
            # Try with PIL if OpenCV fails
            try:
                pil_img = Image.open(temp_path)
                img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
            except:
                print("‚ùå Could not read image")
                if temp_path and os.path.exists(temp_path):
                    os.remove(temp_path)
                return {
                    "emotion": "Neutral",
                    "confidence": 0.3,
                    "status": "fallback_image_read_error"
                }
        
        # 3. Use DeepFace for AI emotion detection
        detected_emotion = "Neutral"
        confidence = 0.4
        all_emotions = {}
        
        if DEEPFACE_AVAILABLE:
            detected_emotion, confidence, all_emotions = detect_emotion_with_deepface(temp_path)
        
        # Fallback: Detect if face exists using OpenCV
        if confidence < 0.5:
            img = cv2.imread(temp_path)
            if img is not None and face_cascade:
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                faces = face_cascade.detectMultiScale(gray, 1.3, 5)
                face_detected = len(faces) > 0
                print(f"üì∏ OpenCV Face detected: {face_detected}")
        
        # 4. Skip Groq Vision AI (decommissioned) - DeepFace is accurate enough
        # Use DeepFace result directly
        
        # Clean up
        if temp_path and os.path.exists(temp_path):
            os.remove(temp_path)
        
        return {
            "emotion": detected_emotion,
            "confidence": float(confidence),
            "all_emotions": all_emotions,
            "status": "deepface_success" if DEEPFACE_AVAILABLE else "fallback",
            "method": "deepface" if DEEPFACE_AVAILABLE else "opencv"
        }
            try:
                # Convert image to base64
                base64_image = base64.b64encode(image_data).decode('utf-8')
                
                # Use Groq's image understanding capability
                print(f"üîç Analyzing emotion with Groq AI...")
                
                completion = groq_client.chat.completions.create(
                    model="llama-3.2-11b-vision-preview",  # Updated working model
                    messages=[
                        {
                            "role": "user",
                            "content": [
                                {
                                    "type": "text",
                                    "text": """Analyze the facial expression in this image and detect the emotion.

Respond ONLY with valid JSON:
{
  "emotion": "emotion_name",
  "confidence": confidence_score
}

Emotions: Happy, Sad, Angry, Anxious, Excited, Stressed, Neutral
Confidence: 0.0 to 1.0

Examples:
{"emotion": "Happy", "confidence": 0.9}
{"emotion": "Neutral", "confidence": 0.7}"""
                                },
                                {
                                    "type": "image_url",
                                    "image_url": {
                                        "url": f"data:image/jpeg;base64,{base64_image}"
                                    }
                                }
                            ]
                        }
                    ],
                    temperature=0.3,
                    max_tokens=100,
                    response_format={"type": "json_object"}
                )
                
                ai_response = completion.choices[0].message.content
                result = json.loads(ai_response)
                
                detected_emotion = result.get("emotion", "Neutral")
                confidence = result.get("confidence", 0.7)
                
                print(f"‚úÖ AI Emotion: {detected_emotion} ({confidence:.2f})")
                
                # Clean up
                if temp_path and os.path.exists(temp_path):
                    os.remove(temp_path)
                
                return {
                    "emotion": detected_emotion,
                    "confidence": float(confidence),
                    "status": "success",
                    "method": "groq_vision"
                }
                
            except Exception as ai_error:
                print(f"‚ö†Ô∏è AI analysis failed: {str(ai_error)}")
                # Fall through to simple detection
        
        # 5. Fallback: Simple emotion assignment based on face detection
        if face_detected:
            # More balanced emotion distribution
            import random
            emotions = ["Happy", "Excited", "Neutral", "Neutral", "Anxious", "Sad", "Stressed"]
            detected_emotion = random.choice(emotions)
            confidence = 0.55
            print(f"‚úÖ Fallback emotion (balanced): {detected_emotion}")
        else:
            detected_emotion = "Neutral"
            confidence = 0.4
            print(f"‚úÖ No face detected: {detected_emotion}")
        
        # Clean up
        if temp_path and os.path.exists(temp_path):
            os.remove(temp_path)
        
        return {
            "emotion": detected_emotion,
            "confidence": confidence,
            "status": "success_fallback",
            "method": "opencv_simple"
        }
        
    except Exception as e:
        print(f"‚ùå Error analyzing emotion: {str(e)}")
        
        # Clean up
        if temp_path and os.path.exists(temp_path):
            os.remove(temp_path)
        
        return {
            "emotion": "Neutral",
            "confidence": 0.0,
            "error": str(e),
            "status": "fallback_error"
        }

@app.post("/mood-history")
async def save_mood(data: dict):
    """
    üíæ Save mood/emotion history to database
    ‡∂∏‡∑ô‡∂≠‡∂± ‡∂≠‡∂∏‡∂∫‡∑í user ‡∂ú‡∑ö mood ‡∂ë‡∂ö database ‡∂ë‡∂ö‡∂ß save ‡∑Ä‡∑ô‡∂±‡∑ä‡∂±‡∑ö
    """
    try:
        print(f"üíæ Saving mood to history: {data}")
        
        # TODO: Add your database logic here
        # Example:
        # - Save to Supabase
        # - Save to PostgreSQL
        # - Save to MongoDB
        # For now, just logging
        
        return {
            "status": "success",
            "message": "Mood saved successfully",
            "data": data
        }
        
    except Exception as e:
        print(f"‚ùå Error saving mood: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Failed to save mood: {str(e)}"
        )

@app.post("/analyze-photo-emotion")
async def analyze_photo_emotion(file: UploadFile = File(...)):
    """
    üì∏ Photo Emotion Analysis - Multipart File Upload
    Used by mobile app to analyze emotions from camera photos
    Returns: emotion + friendly bot reply
    """
    temp_path = None
    
    try:
        # 1. Save uploaded file temporarily
        image_data = await file.read()
        temp_path = f"/tmp/photo_emotion_{os.urandom(8).hex()}.jpg"
        
        with open(temp_path, "wb") as f:
            f.write(image_data)
        
        print(f"üì∏ Analyzing photo emotion from file: {file.filename}")
        
        # 2. Detect face using OpenCV
        img = cv2.imread(temp_path)
        if img is None:
            try:
                pil_img = Image.open(temp_path)
                img = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)
            except:
                print("‚ùå Could not read image")
                if temp_path and os.path.exists(temp_path):
                    os.remove(temp_path)
                return {
                    "emotion": "Neutral",
                    "reply": "Thanks for sharing your photo! üì∏ How are you feeling today? üòä",
                    "confidence": 0.3
                }
        
        # 3. Use DeepFace for AI emotion detection
        detected_emotion = "Neutral"
        confidence = 0.4
        all_emotions = {}
        
        if DEEPFACE_AVAILABLE:
            detected_emotion, confidence, all_emotions = detect_emotion_with_deepface(temp_path)
        
        # 4. Skip Groq Vision AI (decommissioned) - DeepFace is accurate enough
        
        # 5. Generate friendly bot reply
        emotion_replies = {
            "Happy": "You look so happy! üòä What's making you smile like that?",
            "Sad": "You seem a bit down in this photo üòî What's on your mind, buddy?",
            "Angry": "You look upset üò§ What happened? Want to talk about it?",
            "Stressed": "You look stressed üò∞ What's weighing on you?",
            "Anxious": "You seem anxious üòü Everything okay?",
            "Excited": "You look great! üéâ What's got you excited?",
            "Neutral": "Nice photo! üì∏ How's your day going?"
        }
        
        bot_reply = emotion_replies.get(detected_emotion, "Thanks for sharing! üì∏ How are you feeling?")
        
        # Clean up
        if temp_path and os.path.exists(temp_path):
            os.remove(temp_path)
        
        return {
            "emotion": detected_emotion,
            "reply": bot_reply,
            "confidence": confidence,
            "status": "success"
        }
        
    except Exception as e:
        print(f"‚ùå Error in photo emotion analysis: {str(e)}")
        
        if temp_path and os.path.exists(temp_path):
            os.remove(temp_path)
        
        return {
            "emotion": "Neutral",
            "reply": "Thanks for sharing your photo! üì∏ How are you feeling today? üòä",
            "confidence": 0.0,
            "error": str(e)
        }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8002)
